{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m kagglehub\u001b[38;5;241m.\u001b[39mdataset_download(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvikasukani/parkinsons-disease-data-set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vikasukani/parkinsons-disease-data-set\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv from root folder\n",
    "# data preprocessing\n",
    "# train using svm model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM MODEL APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # Read the CSV file\n",
    "# df = pd.read_csv('parkinsons.data')\n",
    "\n",
    "# # Feature selection\n",
    "# X = df.drop(columns=['name', 'status'])\n",
    "# y = df['status']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Feature scaling\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Hyperparameter tuning using GridSearchCV\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'kernel': ['linear', 'rbf'],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "# grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Best parameters\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Train the model with best parameters\n",
    "# best_svm_model = grid_search.best_estimator_\n",
    "# best_svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# # Print confusion matrix and classification report\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# # Step 1: Read the CSV file\n",
    "# df = pd.read_csv('parkinsons.data')\n",
    "\n",
    "# # Step 2: Prepare the data\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=['name', 'status'])  # Drop non-feature columns\n",
    "# y = df['status']  # Target variable\n",
    "\n",
    "# # Step 3: Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 4: Initialize the Random Forest Classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Step 5: Train the model\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 6: Make predictions\n",
    "# y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Step 7: Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# # Step 1: Read the CSV file\n",
    "# df = pd.read_csv('parkinsons.data')\n",
    "\n",
    "# # Step 2: Prepare the data\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=['name', 'status'])  # Drop non-feature columns\n",
    "# y = df['status']  # Target variable\n",
    "\n",
    "# # Step 3: Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 4: Initialize the KNN Classifier\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors based on your needs\n",
    "\n",
    "# # Step 5: Train the model\n",
    "# knn_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 6: Make predictions\n",
    "# y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# # Step 7: Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# # Step 1: Read the CSV file\n",
    "# df = pd.read_csv('parkinsons.data')\n",
    "\n",
    "# # Step 2: Prepare the data\n",
    "# # Separate features and target variable\n",
    "# X = df.drop(columns=['name', 'status'])  # Drop non-feature columns\n",
    "# y = df['status']  # Target variable\n",
    "\n",
    "# # Step 3: Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 4: Initialize the Logistic Regression model\n",
    "# log_reg_model = LogisticRegression(max_iter=1000)  # Increase max_iter if convergence issues occur\n",
    "\n",
    "# # Step 5: Train the model\n",
    "# log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 6: Make predictions\n",
    "# y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# # Step 7: Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib  # Import joblib for saving the model\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('parkinsons.data')\n",
    "\n",
    "# Step 2: Prepare the data\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=['name', 'status'])  # Drop non-feature columns\n",
    "y = df['status']  # Target variable\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize the XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Step 5: Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Step 8: Save the trained model\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "print(\"Model saved as 'xgboost_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import joblib\n",
    "\n",
    "# # Load the saved model\n",
    "# loaded_model = joblib.load('xgboost_model.pkl')\n",
    "\n",
    "# # Function to get user input\n",
    "# def get_user_input():\n",
    "#     print(\"Please enter the following features as a comma-separated list:\")\n",
    "#     print(\"Format: MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz), MDVP:Jitter(%), MDVP:Jitter(Abs), \"\n",
    "#           \"MDVP:RAP, MDVP:PPQ, Jitter:DDP, MDVP:Shimmer, MDVP:Shimmer(dB), Shimmer:APQ3, \"\n",
    "#           \"Shimmer:APQ5, MDVP:APQ, Shimmer:DDA, NHR, HNR, RPDE, DFA, spread1, spread2, D2, PPE\")\n",
    "    \n",
    "#     # Take input as a single line\n",
    "#     input_values = '197.07600,206.89600,192.05500,0.00289,0.00001,0.00166,0.00168,0.00498,0.01098,0.09700,0.00563,0.00680,0.00802,0.01689,0.00339,26.77500,0.422229,0.741367,-7.348300,0.177551,1.743867,0.085569'\n",
    "    \n",
    "#     # Split the input string into a list and convert to float\n",
    "#     input_list = list(map(float, input_values.split(',')))\n",
    "    \n",
    "#     # Create a DataFrame with the input values\n",
    "#     input_data = pd.DataFrame([input_list], columns=['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)',\n",
    "#                                                      'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP',\n",
    "#                                                      'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer',\n",
    "#                                                      'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n",
    "#                                                      'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR',\n",
    "#                                                      'RPDE', 'DFA', 'spread1', 'spread2', 'D2', 'PPE'])\n",
    "#     return input_data\n",
    "\n",
    "# # Get user input\n",
    "# user_input = get_user_input()\n",
    "\n",
    "# # Make a prediction\n",
    "# prediction = loaded_model.predict(user_input)\n",
    "\n",
    "# # Output the prediction\n",
    "# if prediction[0] == 1:\n",
    "#     print(\"Prediction: Parkinson's disease is present.\")\n",
    "# else:\n",
    "#     print(\"Prediction: Parkinson's disease is not present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
